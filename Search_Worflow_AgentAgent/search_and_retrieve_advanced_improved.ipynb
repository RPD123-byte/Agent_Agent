{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=\"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\") # be sure to set your OPENAI_API_KEY environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import builtins\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# Custom print function\n",
    "def wprint(*args, width=70, **kwargs):\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "    wrapped_args = [wrapper.fill(str(arg)) for arg in args]\n",
    "    builtins.print(*wrapped_args, **kwargs)\n",
    "\n",
    "# Function to execute a thread and retrieve the completion\n",
    "def get_completion(message, agent, funcs, thread, client):\n",
    "    # Create new message in the thread\n",
    "    message_response = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=message\n",
    "    )\n",
    "\n",
    "    # Run the thread\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=agent.id,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        # Wait until run completes\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "\n",
    "        if run.status in ['queued', 'in_progress']:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        if run.status == \"requires_action\":\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            tool_outputs = []\n",
    "            for tool_call in tool_calls:\n",
    "                print(f\"Debug: Calling function {tool_call.function.name}\", flush=True)\n",
    "\n",
    "                wprint(f'\\033[31mFunction: {tool_call.function.name}\\033[0m')\n",
    "                func = next((f for f in funcs if f.__name__ == tool_call.function.name), None)\n",
    "                if func:\n",
    "                    try:\n",
    "                        # Assuming arguments are parsed correctly\n",
    "                        func_instance = func(**eval(tool_call.function.arguments))  # Consider safer alternatives to eval\n",
    "                        output = func_instance.run()\n",
    "\n",
    "                        # Ensure output is a string\n",
    "                        if not isinstance(output, str):\n",
    "                            output = str(output)\n",
    "                    except Exception as e:\n",
    "                        output = f\"Error: {e}\"\n",
    "                else:\n",
    "                    output = \"Function not found\"\n",
    "                wprint(f\"\\033[33m{tool_call.function.name}: {output}\\033[0m\")\n",
    "                tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": output})\n",
    "\n",
    "            run = client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id,\n",
    "                tool_outputs=tool_outputs\n",
    "            )\n",
    "        elif run.status == \"failed\":\n",
    "            raise Exception(f\"Run Failed. Error: {run.last_error}\")\n",
    "        else:\n",
    "            messages = client.beta.threads.messages.list(\n",
    "                thread_id=thread.id\n",
    "            )\n",
    "            latest_message = messages.data[0].content[0].text.value\n",
    "            return latest_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose Agent function and all precursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.adapters import openai as lc_openai\n",
    "from colorama import Fore, Style\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_response(model, messages, temperature, max_tokens, llm_provider):\n",
    "    paragraph = \"\"\n",
    "    response = \"\"\n",
    "\n",
    "    for chunk in lc_openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            provider=llm_provider,\n",
    "            stream=True,\n",
    "    ):\n",
    "        content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n",
    "        if content is not None:\n",
    "            response += content\n",
    "            paragraph += content\n",
    "            if \"\\n\" in paragraph:\n",
    "                print(f\"{Fore.GREEN}{paragraph}{Style.RESET_ALL}\")\n",
    "                paragraph = \"\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_chat_completion_request(\n",
    "        messages, model, temperature, max_tokens, stream, llm_provider\n",
    "):\n",
    "    if not stream:\n",
    "        result = lc_openai.ChatCompletion.create(\n",
    "            model=model,  # Change model here to use different models\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            provider=llm_provider,  # Change provider here to use a different API\n",
    "        )\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        return await stream_response(model, messages, temperature, max_tokens, llm_provider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_chat_completion(\n",
    "        messages: list,  # type: ignore\n",
    "        model: Optional[str] = None,\n",
    "        temperature: float = 1.0,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        llm_provider: Optional[str] = None,\n",
    "        stream: Optional[bool] = False,\n",
    ") -> str:\n",
    "    \"\"\"Create a chat completion using the OpenAI API\n",
    "    Args:\n",
    "        messages (list[dict[str, str]]): The messages to send to the chat completion\n",
    "        model (str, optional): The model to use. Defaults to None.\n",
    "        temperature (float, optional): The temperature to use. Defaults to 0.9.\n",
    "        max_tokens (int, optional): The max tokens to use. Defaults to None.\n",
    "        stream (bool, optional): Whether to stream the response. Defaults to False.\n",
    "        llm_provider (str, optional): The LLM Provider to use.\n",
    "        webocket (WebSocket): The websocket used in the currect request\n",
    "    Returns:\n",
    "        str: The response from the chat completion\n",
    "    \"\"\"\n",
    "\n",
    "    # validate input\n",
    "    if model is None:\n",
    "        raise ValueError(\"Model cannot be None\")\n",
    "    if max_tokens is not None and max_tokens > 8001:\n",
    "        raise ValueError(f\"Max tokens cannot be more than 8001, but got {max_tokens}\")\n",
    "\n",
    "    # create response\n",
    "    for attempt in range(10):  # maximum of 10 attempts\n",
    "        response = await send_chat_completion_request(\n",
    "            messages, model, temperature, max_tokens, stream, llm_provider\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    print(\"Failed to get response from OpenAI API\")\n",
    "    raise RuntimeError(\"Failed to get response from OpenAI API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_agent_instructions():\n",
    "    return \"\"\"\n",
    "        This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific server, defined by its type and role, with each server requiring distinct instructions.\n",
    "        Agent\n",
    "        The server is determined by the field of the topic and the specific name of the server that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each server type is associated with a corresponding emoji.\n",
    "\n",
    "        examples:\n",
    "        task: \"should I invest in apple stocks?\"\n",
    "        response: \n",
    "        {\n",
    "            \"server\": \"💰 Finance Agent\",\n",
    "            \"agent_role_prompt: \"You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.\"\n",
    "        }\n",
    "        task: \"could reselling sneakers become profitable?\"\n",
    "        response: \n",
    "        { \n",
    "            \"server\":  \"📈 Business Analyst Agent\",\n",
    "            \"agent_role_prompt\": \"You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.\"\n",
    "        }\n",
    "        task: \"what are the most interesting sites in Tel Aviv?\"\n",
    "        response:\n",
    "        {\n",
    "            \"server:  \"🌍 Travel Agent\",\n",
    "            \"agent_role_prompt\": \"You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.\"\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def choose_agent(query, cfg):\n",
    "    \"\"\"\n",
    "    Chooses the agent automatically\n",
    "    Args:\n",
    "        query: original query\n",
    "        cfg: Config\n",
    "\n",
    "    Returns:\n",
    "        agent: Agent name\n",
    "        agent_role_prompt: Agent role prompt\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await create_chat_completion(\n",
    "            model=cfg.smart_llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{auto_agent_instructions()}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"task: {query}\"}],\n",
    "            temperature=0,\n",
    "            llm_provider=cfg.llm_provider\n",
    "        )\n",
    "        agent_dict = json.loads(response)\n",
    "        return agent_dict[\"server\"], agent_dict[\"agent_role_prompt\"]\n",
    "    except Exception as e:\n",
    "        return \"Default Agent\", \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get context by urls function and all precursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.retrievers import ArxivRetriever\n",
    "from functools import partial\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    \"\"\"\n",
    "    Scraper class to extract the content from the links\n",
    "    \"\"\"\n",
    "    def __init__(self, urls, user_agent):\n",
    "        \"\"\"\n",
    "        Initialize the Scraper class.\n",
    "        Args:\n",
    "            urls:\n",
    "        \"\"\"\n",
    "        self.urls = urls\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": user_agent\n",
    "        })\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Extracts the content from the links\n",
    "        \"\"\"\n",
    "        partial_extract = partial(self.extract_data_from_link, session=self.session)\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            contents = executor.map(partial_extract, self.urls)\n",
    "        res = [content for content in contents if content['raw_content'] is not None]\n",
    "        return res\n",
    "\n",
    "    def extract_data_from_link(self, link, session):\n",
    "        \"\"\"\n",
    "        Extracts the data from the link\n",
    "        \"\"\"\n",
    "        content = \"\"\n",
    "        try:\n",
    "            if link.endswith(\".pdf\"):\n",
    "                content = self.scrape_pdf_with_pymupdf(link)\n",
    "            elif \"arxiv.org\" in link:\n",
    "                doc_num = link.split(\"/\")[-1]\n",
    "                content = self.scrape_pdf_with_arxiv(doc_num)\n",
    "            elif link:\n",
    "                content = self.scrape_text_with_bs(link, session)\n",
    "\n",
    "            if len(content) < 100:\n",
    "                return {'url': link, 'raw_content': None}\n",
    "            return {'url': link, 'raw_content': content}\n",
    "        except Exception as e:\n",
    "            return {'url': link, 'raw_content': None}\n",
    "\n",
    "    def scrape_text_with_bs(self, link, session):\n",
    "        response = session.get(link, timeout=4)\n",
    "        soup = BeautifulSoup(response.content, 'lxml', from_encoding=response.encoding)\n",
    "\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.extract()\n",
    "\n",
    "        raw_content = self.get_content_from_url(soup)\n",
    "        lines = (line.strip() for line in raw_content.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        content = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
    "        return content\n",
    "\n",
    "    def scrape_pdf_with_pymupdf(self, url) -> str:\n",
    "        \"\"\"Scrape a pdf with pymupdf\n",
    "\n",
    "        Args:\n",
    "            url (str): The url of the pdf to scrape\n",
    "\n",
    "        Returns:\n",
    "            str: The text scraped from the pdf\n",
    "        \"\"\"\n",
    "        loader = PyMuPDFLoader(url)\n",
    "        doc = loader.load()\n",
    "        return str(doc)\n",
    "\n",
    "    def scrape_pdf_with_arxiv(self, query) -> str:\n",
    "        \"\"\"Scrape a pdf with arxiv\n",
    "        default document length of 70000 about ~15 pages or None for no limit\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search for\n",
    "\n",
    "        Returns:\n",
    "            str: The text scraped from the pdf\n",
    "        \"\"\"\n",
    "        retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)\n",
    "        docs = retriever.get_relevant_documents(query=query)\n",
    "        return docs[0].page_content\n",
    "\n",
    "    def get_content_from_url(self, soup):\n",
    "        \"\"\"Get the text from the soup\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): The soup to get the text from\n",
    "\n",
    "        Returns:\n",
    "            str: The text from the soup\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5']\n",
    "        for element in soup.find_all(tags):  # Find all the <p> elements\n",
    "            text += element.text + \"\\n\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(urls, cfg=None):\n",
    "    \"\"\"\n",
    "    Scrapes the urls\n",
    "    Args:\n",
    "        urls: List of urls\n",
    "        cfg: Config (optional)\n",
    "\n",
    "    Returns:\n",
    "        text: str\n",
    "\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    user_agent = cfg.user_agent if cfg else \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\"\n",
    "    try:\n",
    "        content = Scraper(urls, user_agent).run()\n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.RED}Error in scrape_urls: {e}{Style.RESET_ALL}\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_context_by_urls(self, urls):\n",
    "    \"\"\"\n",
    "        Scrapes and compresses the context from the given urls\n",
    "    \"\"\"\n",
    "    new_search_urls = await self.get_new_urls(urls)\n",
    "    print(\"logs\", f\"🧠 I will conduct my research based on the following urls: {new_search_urls}...\")\n",
    "    scraped_sites = scrape_urls(new_search_urls, self.cfg)\n",
    "    return await self.get_similar_content_by_query(self.query, scraped_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get context by search function and all precursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do research on: Coffee - History include specific reports on these subtopics: (Origin, Spread of Coffee, Coffee Houses)', 'Do research on: Coffee - Types - Bean Varieties include specific reports on these subtopics: (Arabica, Robusta)', 'Do research on: Coffee - Types - Brewing Methods include specific reports on these subtopics: (Espresso, French Press, Pour-Over, Cold Brew)', 'Do research on: Coffee - Processing include specific reports on these subtopics: (Harvesting, Drying, Roasting)', 'Do research on: Coffee - Attributes include specific reports on these subtopics: (Flavor Profiles, Aroma, Caffeine Content)', 'Do research on: Coffee - Cultural Impact include specific reports on these subtopics: (Ceremonial Use, Social Gatherings, Art and Literature)']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class HierarchicalSearchCreator:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def format_chunks(self, current_data=None, prefix='', depth=0):\n",
    "        results = []\n",
    "        if current_data is None:\n",
    "            current_data = self.data\n",
    "\n",
    "        for key, value in current_data.items():\n",
    "            # Constructing the new prefix based on depth\n",
    "            new_prefix = f\"{prefix} - {key}\" if prefix else key\n",
    "\n",
    "            if isinstance(value, dict):\n",
    "                # For non-leaf nodes, go deeper and combine results\n",
    "                if any(isinstance(subval, dict) for subval in value.values()):\n",
    "                    # If there are deeper dictionaries, continue going deeper\n",
    "                    nested_results = self.format_chunks(value, new_prefix, depth + 1)\n",
    "                    results.extend(nested_results)\n",
    "                else:\n",
    "                    # For leaf nodes, create the formatted string\n",
    "                    subtopics = [f\"{sub_key}\" for sub_key in value.keys()]\n",
    "                    subtopics_str = ', '.join(subtopics)\n",
    "                    formatted_str = f'Do research on: {new_prefix} include specific reports on these subtopics: ({subtopics_str})'\n",
    "                    results.append(formatted_str)\n",
    "            else:\n",
    "                # For leaf nodes at the first level\n",
    "                if depth == 0:\n",
    "                    results.append(f'Do research on: {new_prefix} include specific reports on these subtopics: ({value})')\n",
    "\n",
    "        return results\n",
    "\n",
    "# Load hierarchy from database.json\n",
    "with open('database.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize the processor with loaded data\n",
    "processor = HierarchicalSearchCreator(data)\n",
    "\n",
    "# Format and print the chunks\n",
    "formatted_chunks = processor.format_chunks()\n",
    "print(formatted_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_queries_prompt(question, max_iterations=3):\n",
    "    \"\"\" Generates the search queries prompt for the given question.\n",
    "    Args: question (str): The question to generate the search queries prompt for\n",
    "    Returns: str: The search queries prompt for the given question\n",
    "    \"\"\"\n",
    "\n",
    "    return f'Write {max_iterations} google search queries to search online that form an objective opinion from the following topics: \"{question}\"' \\\n",
    "           f'Use the current date if needed: {datetime.now().strftime(\"%B %d, %Y\")}.\\n' \\\n",
    "           f'You must respond with a list of strings in the following format: [\"query 1\", \"query 2\", \"query 3\"].'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_sub_queries(query, agent_role_prompt, cfg):\n",
    "    \"\"\"\n",
    "    Gets the sub queries\n",
    "    Args:\n",
    "        query: original query\n",
    "        agent_role_prompt: agent role prompt\n",
    "        cfg: Config\n",
    "\n",
    "    Returns:\n",
    "        sub_queries: List of sub queries\n",
    "\n",
    "    \"\"\"\n",
    "    max_research_iterations = cfg.max_iterations if cfg.max_iterations else 1\n",
    "    response = await create_chat_completion(\n",
    "        model=cfg.smart_llm_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": generate_search_queries_prompt(query, max_iterations=max_research_iterations)}],\n",
    "        temperature=0,\n",
    "        llm_provider=cfg.llm_provider\n",
    "    )\n",
    "    sub_queries = json.loads(response)\n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_sites_by_query(self, sub_query):\n",
    "    \"\"\"\n",
    "    Runs a sub-query\n",
    "    Args:\n",
    "        sub_query:\n",
    "\n",
    "    Returns:\n",
    "        Summary\n",
    "    \"\"\"\n",
    "    # Get Urls\n",
    "    retriever = self.retriever(sub_query)\n",
    "    search_results = retriever.search(max_results=self.cfg.max_search_results_per_query)\n",
    "    new_search_urls = await self.get_new_urls([url.get(\"href\") for url in search_results])\n",
    "\n",
    "    # Scrape Urls\n",
    "    # await stream_output(\"logs\", f\"📝Scraping urls {new_search_urls}...\\n\", self.websocket)\n",
    "    print(f\"🤔Researching for relevant information...\\n\")\n",
    "    scraped_content_results = scrape_urls(new_search_urls, self.cfg)\n",
    "    return scraped_content_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "\n",
    "\n",
    "class SearchAPIRetriever(BaseRetriever):\n",
    "    \"\"\"Search API retriever.\"\"\"\n",
    "    pages: List[Dict] = []\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "\n",
    "        docs = [\n",
    "            Document(\n",
    "                page_content=page.get(\"raw_content\", \"\"),\n",
    "                metadata={\n",
    "                    \"title\": page.get(\"title\", \"\"),\n",
    "                    \"source\": page.get(\"url\", \"\"),\n",
    "                },\n",
    "            )\n",
    "            for page in self.pages\n",
    "        ]\n",
    "\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import (\n",
    "    ContextualCompressionRetriever,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import (\n",
    "    DocumentCompressorPipeline,\n",
    "    EmbeddingsFilter,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "class ContextCompressor:\n",
    "    def __init__(self, documents, embeddings, max_results=5, **kwargs):\n",
    "        self.max_results = max_results\n",
    "        self.documents = documents\n",
    "        self.kwargs = kwargs\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def _get_contextual_retriever(self):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings, similarity_threshold=0.78)\n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[splitter, relevance_filter]\n",
    "        )\n",
    "        base_retriever = SearchAPIRetriever(\n",
    "            pages=self.documents\n",
    "        )\n",
    "        contextual_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=pipeline_compressor, base_retriever=base_retriever\n",
    "        )\n",
    "        return contextual_retriever\n",
    "\n",
    "    def _pretty_print_docs(self, docs, top_n):\n",
    "        return f\"\\n\".join(f\"Source: {d.metadata.get('source')}\\n\"\n",
    "                          f\"Title: {d.metadata.get('title')}\\n\"\n",
    "                          f\"Content: {d.page_content}\\n\"\n",
    "                          for i, d in enumerate(docs) if i < top_n)\n",
    "\n",
    "    def get_context(self, query, max_results=5):\n",
    "        compressed_docs = self._get_contextual_retriever()\n",
    "        relevant_docs = compressed_docs.get_relevant_documents(query)\n",
    "        return self._pretty_print_docs(relevant_docs, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_similar_content_by_query(self, query, pages):\n",
    "    print(f\"📃 Getting relevant content based on query: {query}...\")\n",
    "    # Summarize Raw Data\n",
    "    context_compressor = ContextCompressor(documents=pages, embeddings=self.memory.get_embeddings())\n",
    "    # Run Tasks\n",
    "    return context_compressor.get_context(query, max_results=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_context_by_search(self, query):\n",
    "    \"\"\"\n",
    "        Generates the context for the research task by searching the query and scraping the results\n",
    "    Returns:\n",
    "        context: List of context\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    # Generate Sub-Queries including original query\n",
    "    sub_queries = await get_sub_queries(query, self.role, self.cfg) + [query]\n",
    "    print(f\"🧠 I will conduct my research based on the following queries: {sub_queries}...\")\n",
    "\n",
    "    # Run Sub-Queries\n",
    "    for sub_query in sub_queries:\n",
    "        print(f\"\\n🔎 Running research for '{sub_query}'...\")\n",
    "        scraped_sites = await self.scrape_sites_by_query(sub_query)\n",
    "        content = await self.get_similar_content_by_query(sub_query, scraped_sites)\n",
    "        print(f\"📃 {content}\")\n",
    "        context.append(content)\n",
    "\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate report function and precursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_prompt(question, context, report_format=\"apa\", total_words=1000):\n",
    "    \"\"\" Generates the report prompt for the given question and research summary.\n",
    "    Args: question (str): The question to generate the report prompt for\n",
    "            research_summary (str): The research summary to generate the report prompt for\n",
    "    Returns: str: The report prompt for the given question and research summary\n",
    "    \"\"\"\n",
    "\n",
    "    return f'Information: \"\"\"{context}\"\"\"\\n\\n' \\\n",
    "           f'Using the above information, answer the following' \\\n",
    "           f' query or task: \"{question}\" in a detailed report --' \\\n",
    "           \" The report should focus on the answer to the query, should be well structured, informative,\" \\\n",
    "           f\" in depth and comprehensive, with facts and numbers if available and a minimum of {total_words} words.\\n\" \\\n",
    "           \"You should strive to write the report as long as you can using all relevant and necessary information provided.\\n\" \\\n",
    "           \"You must write the report with markdown syntax.\\n \" \\\n",
    "           f\"Use an unbiased and journalistic tone. \\n\" \\\n",
    "           \"You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n\" \\\n",
    "           f\"You MUST write all used source urls at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each.\\n\" \\\n",
    "           f\"You MUST write the report in {report_format} format.\\n \" \\\n",
    "            f\"Cite search results using inline notations. Only cite the most \\\n",
    "            relevant results that answer the query accurately. Place these citations at the end \\\n",
    "            of the sentence or paragraph that reference them.\\n\"\\\n",
    "            f\"Please do your best, this is very important to my career. \" \\\n",
    "            f\"Assume that the current date is {datetime.now().strftime('%B %d, %Y')}\"\n",
    "\n",
    "\n",
    "def generate_resource_report_prompt(question, context, report_format=\"apa\", total_words=1000):\n",
    "    \"\"\"Generates the resource report prompt for the given question and research summary.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to generate the resource report prompt for.\n",
    "        context (str): The research summary to generate the resource report prompt for.\n",
    "\n",
    "    Returns:\n",
    "        str: The resource report prompt for the given question and research summary.\n",
    "    \"\"\"\n",
    "    return f'\"\"\"{context}\"\"\"\\n\\nBased on the above information, generate a bibliography recommendation report for the following' \\\n",
    "           f' question or topic: \"{question}\". The report should provide a detailed analysis of each recommended resource,' \\\n",
    "           ' explaining how each source can contribute to finding answers to the research question.\\n' \\\n",
    "           'Focus on the relevance, reliability, and significance of each source.\\n' \\\n",
    "           'Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax.\\n' \\\n",
    "           'Include relevant facts, figures, and numbers whenever available.\\n' \\\n",
    "           'The report should have a minimum length of 700 words.\\n' \\\n",
    "            'You MUST include all relevant source urls.'\n",
    "\n",
    "def generate_custom_report_prompt(query_prompt, context, report_format=\"apa\", total_words=1000):\n",
    "    return f'\"{context}\"\\n\\n{query_prompt}'\n",
    "\n",
    "\n",
    "def generate_outline_report_prompt(question, context, report_format=\"apa\", total_words=1000):\n",
    "    \"\"\" Generates the outline report prompt for the given question and research summary.\n",
    "    Args: question (str): The question to generate the outline report prompt for\n",
    "            research_summary (str): The research summary to generate the outline report prompt for\n",
    "    Returns: str: The outline report prompt for the given question and research summary\n",
    "    \"\"\"\n",
    "\n",
    "    return f'\"\"\"{context}\"\"\" Using the above information, generate an outline for a research report in Markdown syntax' \\\n",
    "           f' for the following question or topic: \"{question}\". The outline should provide a well-structured framework' \\\n",
    "           ' for the research report, including the main sections, subsections, and key points to be covered.' \\\n",
    "           ' The research report should be detailed, informative, in-depth, and a minimum of 1,200 words.' \\\n",
    "           ' Use appropriate Markdown syntax to format the outline and ensure readability.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_by_type(report_type):\n",
    "    report_type_mapping = {\n",
    "        'research_report': generate_report_prompt,\n",
    "        'resource_report': generate_resource_report_prompt,\n",
    "        'outline_report': generate_outline_report_prompt,\n",
    "        'custom_report': generate_custom_report_prompt\n",
    "    }\n",
    "    return report_type_mapping[report_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_report(query, context, agent_role_prompt, report_type, cfg):\n",
    "    \"\"\"\n",
    "    generates the final report\n",
    "    Args:\n",
    "        query:\n",
    "        context:\n",
    "        agent_role_prompt:\n",
    "        report_type:\n",
    "        cfg:\n",
    "\n",
    "    Returns:\n",
    "        report:\n",
    "\n",
    "    \"\"\"\n",
    "    generate_prompt = get_report_by_type(report_type)\n",
    "    report = \"\"\n",
    "    try:\n",
    "        report = await create_chat_completion(\n",
    "            model=cfg.smart_llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{generate_prompt(query, context, cfg.report_format, cfg.total_words)}\"}],\n",
    "            temperature=0,\n",
    "            llm_provider=cfg.llm_provider,\n",
    "            stream=True,\n",
    "            max_tokens=cfg.smart_token_limit\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.RED}Error in generate_report: {e}{Style.RESET_ALL}\")\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily API Retriever\n",
    "\n",
    "# libraries\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class TavilySearch():\n",
    "    \"\"\"\n",
    "    Tavily API Retriever\n",
    "    \"\"\"\n",
    "    def __init__(self, query):\n",
    "        \"\"\"\n",
    "        Initializes the TavilySearch object\n",
    "        Args:\n",
    "            query:\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.api_key = self.get_api_key()\n",
    "        self.client = TavilyClient(self.api_key)\n",
    "\n",
    "    def get_api_key(self):\n",
    "        \"\"\"\n",
    "        Gets the Tavily API key\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Get the API key\n",
    "        try:\n",
    "            api_key = os.environ[\"TAVILY_API_KEY\"]\n",
    "        except:\n",
    "            raise Exception(\"Tavily API key not found. Please set the TAVILY_API_KEY environment variable. \"\n",
    "                            \"You can get a key at https://app.tavily.com\")\n",
    "        return api_key\n",
    "\n",
    "    def search(self, max_results=7):\n",
    "        \"\"\"\n",
    "        Searches the query\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search the query\n",
    "            results = self.client.search(self.query, search_depth=\"advanced\", max_results=max_results)\n",
    "            # Return the results\n",
    "            search_response = [{\"href\": obj[\"url\"], \"body\": obj[\"content\"]} for obj in results.get(\"results\", [])]\n",
    "        except Exception as e: # Fallback in case overload on Tavily Search API\n",
    "            ddg = DDGS()\n",
    "            search_response = ddg.text(self.query, region='wt-wt', max_results=max_results)\n",
    "        return search_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily API Retriever\n",
    "\n",
    "# libraries\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "class TavilyNews():\n",
    "    \"\"\"\n",
    "    Tavily News API Retriever\n",
    "    Retrieve news articles from the Tavily News API\n",
    "    \"\"\"\n",
    "    def __init__(self, query):\n",
    "        \"\"\"\n",
    "        Initializes the TavilySearch object\n",
    "        Args:\n",
    "            query:\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.api_key = self.get_api_key()\n",
    "        self.client = TavilyClient(self.api_key)\n",
    "\n",
    "    def get_api_key(self):\n",
    "        \"\"\"\n",
    "        Gets the Tavily API key\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Get the API key\n",
    "        try:\n",
    "            api_key = os.environ[\"TAVILY_API_KEY\"]\n",
    "        except:\n",
    "            raise Exception(\"Tavily API key not found. Please set the TAVILY_API_KEY environment variable. \"\n",
    "                            \"You can get a key at https://app.tavily.com\")\n",
    "        return api_key\n",
    "\n",
    "    def search(self, max_results=7):\n",
    "        \"\"\"\n",
    "        Searches the query\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Search the query\n",
    "        results = self.client.search(self.query, search_depth=\"advanced\", topic=\"news\", max_results=max_results)\n",
    "        # Return the results\n",
    "        search_response = [{\"href\": obj[\"url\"], \"body\": obj[\"content\"]} for obj in results.get(\"results\", [])]\n",
    "        return search_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_prompt(query, data):\n",
    "    \"\"\" Generates the summary prompt for the given question and text.\n",
    "    Args: question (str): The question to generate the summary prompt for\n",
    "            text (str): The text to generate the summary prompt for\n",
    "    Returns: str: The summary prompt for the given question and text\n",
    "    \"\"\"\n",
    "\n",
    "    return f'{data}\\n Using the above text, summarize it based on the following task or query: \"{query}\".\\n If the ' \\\n",
    "           f'query cannot be answered using the text, YOU MUST summarize the text in short.\\n Include all factual ' \\\n",
    "           f'information such as numbers, stats, quotes, etc if available. '\n",
    "\n",
    "def get_retriever(retriever):\n",
    "    \"\"\"\n",
    "    Gets the retriever\n",
    "    Args:\n",
    "        retriever: retriever name\n",
    "\n",
    "    Returns:\n",
    "        retriever: Retriever class\n",
    "\n",
    "    \"\"\"\n",
    "    match retriever:\n",
    "        case \"tavily\":\n",
    "            retriever = TavilySearch\n",
    "        case \"tavily_news\":\n",
    "            retriever = TavilyNews\n",
    "\n",
    "        case _:\n",
    "            raise Exception(\"Retriever not found.\")\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def scrape_urls(urls, cfg=None):\n",
    "    \"\"\"\n",
    "    Scrapes the urls\n",
    "    Args:\n",
    "        urls: List of urls\n",
    "        cfg: Config (optional)\n",
    "\n",
    "    Returns:\n",
    "        text: str\n",
    "\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    user_agent = cfg.user_agent if cfg else \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\"\n",
    "    try:\n",
    "        content = Scraper(urls, user_agent).run()\n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.RED}Error in scrape_urls: {e}{Style.RESET_ALL}\")\n",
    "    return content\n",
    "\n",
    "\n",
    "async def summarize(query, content, agent_role_prompt, cfg):\n",
    "    \"\"\"\n",
    "    Asynchronously summarizes a list of URLs.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        content (list): List of dictionaries with 'url' and 'raw_content'.\n",
    "        agent_role_prompt (str): The role prompt for the agent.\n",
    "        cfg (object): Configuration object.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with 'url' and 'summary'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to handle each summarization task for a chunk\n",
    "    async def handle_task(url, chunk):\n",
    "        summary = await summarize_url(query, chunk, agent_role_prompt, cfg)\n",
    "        if summary:\n",
    "            print(f\"🌐 Summarizing url: {url}\")\n",
    "            print(\"logs\", f\"📃 {summary}\")\n",
    "        return url, summary\n",
    "\n",
    "    # Function to split raw content into chunks of 10,000 words\n",
    "    def chunk_content(raw_content, chunk_size=10000):\n",
    "        words = raw_content.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            yield ' '.join(words[i:i+chunk_size])\n",
    "\n",
    "    # Process each item one by one, but process chunks in parallel\n",
    "    concatenated_summaries = []\n",
    "    for item in content:\n",
    "        url = item['url']\n",
    "        raw_content = item['raw_content']\n",
    "\n",
    "        # Create tasks for all chunks of the current URL\n",
    "        chunk_tasks = [handle_task(url, chunk) for chunk in chunk_content(raw_content)]\n",
    "\n",
    "        # Run chunk tasks concurrently\n",
    "        chunk_summaries = await asyncio.gather(*chunk_tasks)\n",
    "\n",
    "        # Aggregate and concatenate summaries for the current URL\n",
    "        summaries = [summary for _, summary in chunk_summaries if summary]\n",
    "        concatenated_summary = ' '.join(summaries)\n",
    "        concatenated_summaries.append({'url': url, 'summary': concatenated_summary})\n",
    "\n",
    "    return concatenated_summaries\n",
    "\n",
    "\n",
    "async def summarize_url(query, raw_data, agent_role_prompt, cfg):\n",
    "    \"\"\"\n",
    "    Summarizes the text\n",
    "    Args:\n",
    "        query:\n",
    "        raw_data:\n",
    "        agent_role_prompt:\n",
    "        cfg:\n",
    "\n",
    "    Returns:\n",
    "        summary: str\n",
    "\n",
    "    \"\"\"\n",
    "    summary = \"\"\n",
    "    try:\n",
    "        summary = await create_chat_completion(\n",
    "            model=cfg.fast_llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{generate_summary_prompt(query, raw_data)}\"}],\n",
    "            temperature=0,\n",
    "            llm_provider=cfg.llm_provider\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.RED}Error in summarize: {e}{Style.RESET_ALL}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "async def stream_output(type, output, websocket=None, logging=True):\n",
    "    \"\"\"\n",
    "    Streams output to the websocket\n",
    "    Args:\n",
    "        type:\n",
    "        output:\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not websocket or logging:\n",
    "        print(output)\n",
    "\n",
    "    if websocket:\n",
    "        await websocket.send_json({\"type\": type, \"output\": output})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Config class for GPT Researcher.\"\"\"\n",
    "\n",
    "    def __init__(self, config_file: str = None):\n",
    "        \"\"\"Initialize the config class.\"\"\"\n",
    "        self.config_file = config_file if config_file else os.getenv('CONFIG_FILE')\n",
    "        self.retriever = os.getenv('SEARCH_RETRIEVER', \"tavily\")\n",
    "        self.llm_provider = os.getenv('LLM_PROVIDER', \"ChatOpenAI\")\n",
    "        self.fast_llm_model = os.getenv('FAST_LLM_MODEL', \"gpt-3.5-turbo-16k\")\n",
    "        self.smart_llm_model = os.getenv('SMART_LLM_MODEL', \"gpt-4-1106-preview\")\n",
    "        self.fast_token_limit = int(os.getenv('FAST_TOKEN_LIMIT', 2000))\n",
    "        self.smart_token_limit = int(os.getenv('SMART_TOKEN_LIMIT', 4000))\n",
    "        self.browse_chunk_max_length = int(os.getenv('BROWSE_CHUNK_MAX_LENGTH', 8192))\n",
    "        self.summary_token_limit = int(os.getenv('SUMMARY_TOKEN_LIMIT', 700))\n",
    "        self.temperature = float(os.getenv('TEMPERATURE', 0.55))\n",
    "        self.user_agent = os.getenv('USER_AGENT', \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                                                   \"(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\")\n",
    "        self.max_search_results_per_query = int(os.getenv('MAX_SEARCH_RESULTS_PER_QUERY', 5))\n",
    "        self.memory_backend = os.getenv('MEMORY_BACKEND', \"local\")\n",
    "        self.total_words = int(os.getenv('TOTAL_WORDS', 1000))\n",
    "        self.report_format = os.getenv('REPORT_FORMAT', \"APA\")\n",
    "        self.max_iterations = int(os.getenv('MAX_ITERATIONS', 3))\n",
    "        self.agent_role = os.getenv('AGENT_ROLE', None)\n",
    "\n",
    "        self.load_config_file()\n",
    "\n",
    "    def load_config_file(self) -> None:\n",
    "        \"\"\"Load the config file.\"\"\"\n",
    "        if self.config_file is None:\n",
    "            return None\n",
    "        with open(self.config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        for key, value in config.items():\n",
    "            self.__dict__[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self._embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class GPTResearcher:\n",
    "    \"\"\"\n",
    "    GPT Researcher\n",
    "    \"\"\"\n",
    "    def __init__(self, query, report_type, source_urls=None, config_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the GPT Researcher class.\n",
    "        Args:\n",
    "            query:\n",
    "            report_type:\n",
    "            config_path:\n",
    "            websocket:\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.agent = None\n",
    "        self.role = None\n",
    "        self.report_type = report_type\n",
    "        self.cfg = Config(config_path)\n",
    "        self.retriever = get_retriever(self.cfg.retriever)\n",
    "        self.context = []\n",
    "        self.source_urls = source_urls\n",
    "        self.memory = Memory()\n",
    "        self.visited_urls = set()\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"\n",
    "        Runs the GPT Researcher\n",
    "        Returns:\n",
    "            Report\n",
    "        \"\"\"\n",
    "        print(f\"🔎 Running research for '{self.query}'...\")\n",
    "        # Generate Agent\n",
    "        self.agent, self.role = await choose_agent(self.query, self.cfg)\n",
    "        print(f\"Logs: {self.agent}\")\n",
    "\n",
    "        # If specified, the researcher will use the given urls as the context for the research.\n",
    "        if self.source_urls:\n",
    "            self.context = await self.get_context_by_urls(self.source_urls)\n",
    "        else:\n",
    "            self.context = await self.get_context_by_search(self.query)\n",
    "\n",
    "        # Write Research Report\n",
    "        if self.report_type == \"custom_report\":\n",
    "            self.role = self.cfg.agent_role if self.cfg.agent_role else self.role\n",
    "        print(f\"✍️ Writing {self.report_type} for research task: {self.query}...\")\n",
    "        report = await generate_report(query=self.query, context=self.context, agent_role_prompt=self.role, report_type=self.report_type, cfg=self.cfg)\n",
    "        time.sleep(2)\n",
    "        return report\n",
    "    \n",
    "    async def get_context_by_urls(self, urls):\n",
    "        \"\"\"\n",
    "            Scrapes and compresses the context from the given urls\n",
    "        \"\"\"\n",
    "        new_search_urls = await self.get_new_urls(urls)\n",
    "        print(\"logs\", f\"🧠 I will conduct my research based on the following urls: {new_search_urls}...\")\n",
    "        scraped_sites = scrape_urls(new_search_urls, self.cfg)\n",
    "        return await self.get_similar_content_by_query(self.query, scraped_sites)\n",
    "\n",
    "    async def get_context_by_search(self, query):\n",
    "        \"\"\"\n",
    "           Generates the context for the research task by searching the query and scraping the results\n",
    "        Returns:\n",
    "            context: List of context\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        # Generate Sub-Queries including original query\n",
    "        sub_queries = await get_sub_queries(query, self.role, self.cfg) + [query]\n",
    "        print(f\"🧠 I will conduct my research based on the following queries: {sub_queries}...\")\n",
    "\n",
    "        # Run Sub-Queries\n",
    "        for sub_query in sub_queries:\n",
    "            print(f\"\\n🔎 Running research for '{sub_query}'...\")\n",
    "            scraped_sites = await self.scrape_sites_by_query(sub_query)\n",
    "            content = await self.get_similar_content_by_query(sub_query, scraped_sites)\n",
    "            print(f\"📃 {content}\")\n",
    "            context.append(content)\n",
    "\n",
    "        return context\n",
    "\n",
    "    async def get_new_urls(self, url_set_input):\n",
    "        \"\"\" Gets the new urls from the given url set.\n",
    "        Args: url_set_input (set[str]): The url set to get the new urls from\n",
    "        Returns: list[str]: The new urls from the given url set\n",
    "        \"\"\"\n",
    "\n",
    "        new_urls = []\n",
    "        for url in url_set_input:\n",
    "            if url not in self.visited_urls:\n",
    "                print(f\"✅ Adding source url to research: {url}\\n\")\n",
    "\n",
    "                self.visited_urls.add(url)\n",
    "                new_urls.append(url)\n",
    "\n",
    "        return new_urls\n",
    "\n",
    "    async def scrape_sites_by_query(self, sub_query):\n",
    "        \"\"\"\n",
    "        Runs a sub-query\n",
    "        Args:\n",
    "            sub_query:\n",
    "\n",
    "        Returns:\n",
    "            Summary\n",
    "        \"\"\"\n",
    "        # Get Urls\n",
    "        retriever = self.retriever(sub_query)\n",
    "        search_results = retriever.search(max_results=self.cfg.max_search_results_per_query)\n",
    "        new_search_urls = await self.get_new_urls([url.get(\"href\") for url in search_results])\n",
    "\n",
    "        # Scrape Urls\n",
    "        # await stream_output(\"logs\", f\"📝Scraping urls {new_search_urls}...\\n\", self.websocket)\n",
    "        print(f\"🤔Researching for relevant information...\\n\")\n",
    "        scraped_content_results = scrape_urls(new_search_urls, self.cfg)\n",
    "        return scraped_content_results\n",
    "\n",
    "    async def get_similar_content_by_query(self, query, pages):\n",
    "        print(f\"📃 Getting relevant content based on query: {query}...\")\n",
    "        # Summarize Raw Data\n",
    "        context_compressor = ContextCompressor(documents=pages, embeddings=self.memory.get_embeddings())\n",
    "        # Run Tasks\n",
    "        return context_compressor.get_context(query, max_results=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['TAVILY_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# async def main():\n",
    "#     query = \"coffee\"\n",
    "#     report_type = \"research_report\"\n",
    "\n",
    "#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=None, config_path=\"config.json\")\n",
    "#     report = await researcher.run()\n",
    "\n",
    "#     return report  # Return the report\n",
    "\n",
    "\n",
    "# report = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class hierarchy_to_json_tool:\n",
    "    openai_schema = {\n",
    "        \"name\": \"hierarchy_to_json_tool\",\n",
    "        \"description\": \"Converts a hierarchy string into a JSON file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"hierarchy_data\": {\"type\": \"string\", \"description\": \"String representation of the hierarchy dictionary\"}\n",
    "            },\n",
    "            \"required\": [\"hierarchy_data\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, hierarchy_data):\n",
    "        self.hierarchy_data = hierarchy_data\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            # Attempt to parse the input string into a dictionary\n",
    "            hierarchy_dict = json.loads(self.hierarchy_data)\n",
    "\n",
    "            # Write the hierarchy to a JSON file\n",
    "            with open('database.json', 'w') as file:\n",
    "                json.dump(hierarchy_dict, file, indent=4)\n",
    "\n",
    "            return {\"output\": \"database.json created successfully\"}\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            return {\"error\": f\"JSON decoding error: {str(e)}\"}\n",
    "\n",
    "# # Corrected example usage\n",
    "# hierarchy_string = '{\"level1\": {\"sublevel1\": {\"subsublevel1\": \"value1\", \"subsublevel2\": \"value2\"}, \"sublevel2\": {\"subsublevel1\": \"value3\", \"subsublevel2\": \"value4\"}}}'\n",
    "# tool = hierarchy_to_json_tool(hierarchy_data=hierarchy_string)\n",
    "# result = tool.run()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_creation_tools = [hierarchy_to_json_tool]\n",
    "\n",
    "database_creation_agent = client.beta.assistants.create(\n",
    "    name='Database Creation Agent',\n",
    "    instructions=\"\"\"\n",
    "    As a database creation agent, your primary task is to create a database structure for a specified topic. \n",
    "    Make sure the database you create is extremely detailed covering all relevant information about the given topic.\n",
    "    Start by creating a dictionary inside of a string to represent the database hierarchy. \n",
    "    Then, utilize the 'hierarchy_to_json_tool' to process this hierarchy. The 'hierarchy_to_json_tool' must be used everytime. \n",
    "    For 'hierarchy_data', input the string containing the dictionary for the hierarchy.\n",
    "    Here's an example of the hierarchy format:\n",
    "    '{\n",
    "        \"level1\": {\n",
    "            \"sublevel1\": {\n",
    "                \"subsublevel1\": \"value1\",\n",
    "                \"subsublevel2\": \"value2\"\n",
    "            },\n",
    "            ...\n",
    "        },\n",
    "        ...\n",
    "    }'\n",
    "    Make sure to leave placeholder values like \"value\" in the hierarchy to be replaced with detailed information by someone else in the future.\n",
    "    After the tool processes the data, confirm completion by printing \"Successfully Completed\". \n",
    "    \"\"\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\": \"function\", \"function\": hierarchy_to_json_tool.openai_schema},\n",
    "\n",
    "           ]\n",
    ")\n",
    "\n",
    "# # Create a new thread\n",
    "# thread = client.beta.threads.create()\n",
    "\n",
    "# # Main loop for user interaction\n",
    "# # user_input = input(\"User: \")\n",
    "# user_input = \"Give me the database for everything to know about coffee. Make sure its extremely detailed\"\n",
    "# message = get_completion(user_input, database_creation_agent, database_creation_tools, thread, client)\n",
    "# wprint(f\"\\033[34m{database_creation_agent.name}: {message}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(content):\n",
    "    \"\"\"\n",
    "    Writes the provided string content to query.txt, overwriting existing content.\n",
    "\n",
    "    Parameters:\n",
    "    content (str): The content to be written to the file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open('query.txt', 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# write_to_file(\"This is a sample string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "class report_generator_tool:\n",
    "    openai_schema = {\n",
    "        \"name\": \"report_generator_tool\",\n",
    "        \"description\": \"Generates a research report based on a given query\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open('query.txt', 'r') as file:\n",
    "                self.query = file.read().strip()\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error reading query from query.txt: {str(e)}\")\n",
    "        self.report_type = \"research_report\"\n",
    "\n",
    "    async def run_async(self):\n",
    "        try:\n",
    "            researcher = GPTResearcher(query=self.query, report_type=self.report_type, source_urls=None, config_path=\"config.json\")\n",
    "            report = await researcher.run()\n",
    "\n",
    "            # Write the report directly to report.txt\n",
    "            with open('report.txt', 'w') as file:\n",
    "                file.write(report)  # Assuming report is a string\n",
    "\n",
    "            return {\"output\": \"report.txt created successfully with the report\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error occurred: {str(e)}\"}\n",
    "\n",
    "    def run(self):\n",
    "        # Apply nest_asyncio to enable nesting event loops\n",
    "        nest_asyncio.apply()\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = loop.run_until_complete(self.run_async())\n",
    "        return result\n",
    "\n",
    "# # Example usage\n",
    "# tool = report_generator_tool(query=\"coffee\")\n",
    "# report_result = tool.run()\n",
    "# print(report_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class string_transformer_tool:\n",
    "    openai_schema = {\n",
    "        \"name\": \"string_transformer_tool\",\n",
    "        \"description\": \"Transforms a specific string format into a list of strings and writes them to a file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open('query.txt', 'r') as file:\n",
    "                self.query = file.read().strip()\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error reading query from query.txt: {str(e)}\")\n",
    "    def run(self):\n",
    "        # Extract the main topic\n",
    "        main_topic = self.query.split(\"include specific reports on these subtopics:\")[0].split(\":\")[1].strip()\n",
    "        \n",
    "        # Check if subtopics exist\n",
    "        if \"(\" in self.query and \")\" in self.query:\n",
    "            # Extract subtopics, remove parentheses and split\n",
    "            subtopics_str = self.query.split(\"(\")[1].split(\")\")[0]\n",
    "            subtopics = subtopics_str.split(\", \") if subtopics_str else []\n",
    "        else:\n",
    "            subtopics = []\n",
    "\n",
    "        # Create the list with formatted strings\n",
    "        result = [f'{main_topic} - {subtopic}' for subtopic in subtopics] if subtopics else [main_topic]\n",
    "\n",
    "        # Write the list to entries.txt\n",
    "        with open('entries.txt', 'w') as file:\n",
    "            for item in result:\n",
    "                file.write(f\"{item}\\n\")\n",
    "\n",
    "        return {\"output\": \"entries.txt updated successfully\"}\n",
    "\n",
    "# input_query = \"Do research on: Coffee - Types - Arabica include specific reports on these subtopics: ()\"\n",
    "# tool = string_transformer_tool()\n",
    "# result = tool.run()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class hierarchy_updater_tool:\n",
    "    openai_schema = {\n",
    "        \"name\": \"hierarchy_updater_tool\",\n",
    "        \"description\": \"Updates a specific place in the hierarchy with the content of information.txt\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"hierarchy_string\": {\"type\": \"string\", \"description\": \"The hierarchy string to be updated\"}\n",
    "            },\n",
    "            \"required\": [\"hierarchy_string\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    def __init__(self, hierarchy_string):\n",
    "        self.hierarchy_string = hierarchy_string\n",
    "    \n",
    "    def run(self):\n",
    "        try:\n",
    "            # Load the hierarchy data\n",
    "            with open('database.json') as file:\n",
    "                hierarchy_data = json.load(file)\n",
    "\n",
    "            # Read the text from information.txt\n",
    "            with open('information.txt') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Update the hierarchy\n",
    "            keys = self.hierarchy_string.strip(':').split(' - ')\n",
    "            data = hierarchy_data\n",
    "            for key in keys[:-1]:\n",
    "                data = data.get(key, {})\n",
    "\n",
    "            if keys[-1] in data:\n",
    "                data[keys[-1]] = text\n",
    "            else:\n",
    "                return f\"Key '{keys[-1]}' not found in the hierarchy\"\n",
    "\n",
    "            # Save the updated hierarchy\n",
    "            with open('database.json', 'w') as file:\n",
    "                json.dump(hierarchy_data, file, indent=4)\n",
    "\n",
    "            return \"Hierarchy updated successfully.\"\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            return f\"File not found error: {str(e)}\"\n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"JSON decoding error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "        \n",
    "\n",
    "# updater = hierarchy_updater_tool('coffee - types - brewed - espresso')\n",
    "\n",
    "# # Use the instance to call the run method\n",
    "# result = updater.run()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_to_file_writer:\n",
    "    openai_schema = {\n",
    "        \"name\": \"text_to_file_writer\",\n",
    "        \"description\": \"Writes provided text to information.txt\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"string\", \"description\": \"Text to be written to the file\"}\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            with open('information.txt', 'w') as file:\n",
    "                file.write(self.text)\n",
    "            return {\"output\": \"information.txt updated successfully\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error occurred while writing to the file: {str(e)}\"}\n",
    "\n",
    "# # Example usage\n",
    "# text = \"This is the text that needs to be written to the file.\"\n",
    "# writer = TextToFileWriter(text)\n",
    "# result = writer.write_to_file()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class entry_processor:\n",
    "    openai_schema = {\n",
    "        \"name\": \"entry_processor\",\n",
    "        \"description\": \"Reads, prints, and deletes the first entry from a specified text file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, filename=\"entries.txt\"):\n",
    "        self.filename = filename\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            with open(self.filename, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            if not lines:\n",
    "                return {\"error\": \"The file is empty or does not exist.\"}\n",
    "\n",
    "            first_entry = lines.pop(0).strip()\n",
    "            print(f\"First entry: {first_entry}\")\n",
    "\n",
    "            with open(self.filename, 'w') as file:\n",
    "                file.writelines(lines)\n",
    "\n",
    "            return {\"output\": f\"Processed and removed the first entry: {first_entry}\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"There are no more database entries to update\"}\n",
    "\n",
    "# # Example usage\n",
    "# processor = entry_processor('entries.txt')\n",
    "# result = processor.run()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class report_reader:\n",
    "    openai_schema = {\n",
    "        \"name\": \"report_reader\",\n",
    "        \"description\": \"Reads, prints, and deletes the first entry from a specified text file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, filename='report.txt'):\n",
    "        self.filename = filename\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            with open(self.filename, 'r') as file:\n",
    "                content = file.read()\n",
    "                print(content)\n",
    "            return {\"output\": \"Content of report.txt printed successfully\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error occurred while reading the file: {str(e)}\"}\n",
    "\n",
    "# # Example usage\n",
    "# reader = report_reader()\n",
    "# result = reader.read_and_print()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_generation_tools = [report_generator_tool, string_transformer_tool, hierarchy_updater_tool, text_to_file_writer, entry_processor, report_reader]\n",
    "\n",
    "report_generation_agent = client.beta.assistants.create(\n",
    "    name='Report Generation Agent',\n",
    "    instructions=\"\"\"\n",
    "    As a Report Generation agent, your primary task is to create a report for a topic given by the user. \n",
    "    To generate a report, use the 'report_generator_tool'.\n",
    "    For the 'query' paramater, input the exact string given by the user.\n",
    "    Now use the 'string_transformer_tool' to get the parts of the database that you must fill with the generated report.\n",
    "    For the 'query' paramater, input the exact string given by the user.\n",
    "    While 'entry_processor' output is not \"There are no more database entries to update\":\n",
    "        Use 'entry_processer' to get the first database entry to update. \n",
    "        If the 'entry_processor' is giving an error try running 'string_transformer_tool' and then try using 'entry_processor' again.\n",
    "        Use 'report_reader' to print the previously generated report.\n",
    "        Now decide what part of the report is relevant and provides valuable information to update the database entry with.\n",
    "        So find whatever information speaks of the topic that was outputted by 'entry_processor'.\n",
    "        Once you determine what's useful, create a massive string with all the relevant information cut out from the report given by 'report_reader' and use 'text_to_file_writer' to send that information to the information.txt file.\n",
    "        Now use 'hierarchy_updater_tool' to update the database entry with the information from the information.txt file.\n",
    "        For the 'hierarchy_string' paramater, input the exact string returned by 'entry_processor'.\n",
    "    \n",
    "    Repeat the While loop until 'entry_processor' output is \"There are no more database entries to update\".\n",
    "    If in this process you realize that the report doesn't have any relevant information for one of the database entries, rerun 'report_generator_tool' and input the database entry with no relevant information as a string to the 'query' paramater and redo the loop.\n",
    "    \"\"\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\": \"function\", \"function\": report_generator_tool.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": string_transformer_tool.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": hierarchy_updater_tool.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": text_to_file_writer.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": entry_processor.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": report_reader.openai_schema},\n",
    "           ]\n",
    ")\n",
    "\n",
    "# # Create a new thread\n",
    "# thread = client.beta.threads.create()\n",
    "\n",
    "# # Main loop for user interaction\n",
    "# # user_input = input(\"User: \")\n",
    "# user_input = \"coffee\"\n",
    "# message = get_completion(user_input, report_generation_agent, report_generation_tools, thread, client)\n",
    "# wprint(f\"\\033[34m{report_generation_agent.name}: {message}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Calling function hierarchy_to_json_tool\n",
      "\u001b[31mFunction: hierarchy_to_json_tool\u001b[0m\n",
      "\u001b[33mhierarchy_to_json_tool: {'output': 'database.json created\n",
      "successfully'}\u001b[0m\n",
      "\u001b[34mDatabase Creation Agent: Successfully Completed\u001b[0m\n",
      "Do research on: coffee - varieties - arabica - flavor_profile include specific reports on these subtopics: (acidity, body, aroma)\n",
      "Debug: Calling function report_generator_tool\n",
      "\u001b[31mFunction: report_generator_tool\u001b[0m\n",
      "🔎 Running research for 'Do research on: coffee - varieties - arabica - flavor_profile include specific reports on these subtopics: (acidity, body, aroma)'...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m thread \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mbeta\u001b[39m.\u001b[39mthreads\u001b[39m.\u001b[39mcreate()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m user_input \u001b[39m=\u001b[39m chunk\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m message \u001b[39m=\u001b[39m get_completion(user_input, report_generation_agent, report_generation_tools, thread, client)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m wprint(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[34m\u001b[39m\u001b[39m{\u001b[39;00mreport_generation_agent\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Assuming arguments are parsed correctly\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     func_instance \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39meval\u001b[39m(tool_call\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39marguments))  \u001b[39m# Consider safer alternatives to eval\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     output \u001b[39m=\u001b[39m func_instance\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# Ensure output is a string\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(output, \u001b[39mstr\u001b[39m):\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m nest_asyncio\u001b[39m.\u001b[39mapply()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_event_loop()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m result \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39;49mrun_until_complete(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_async())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     handle \u001b[39m=\u001b[39m ready\u001b[39m.\u001b[39mpopleft()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handle\u001b[39m.\u001b[39m_cancelled:\n\u001b[0;32m--> 129\u001b[0m         handle\u001b[39m.\u001b[39;49m_run()\n\u001b[1;32m    130\u001b[0m handle \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callback, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args)\n\u001b[1;32m     81\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mSystemExit\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    203\u001b[0m curr_task \u001b[39m=\u001b[39m curr_tasks\u001b[39m.\u001b[39mget(task\u001b[39m.\u001b[39m_loop)\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     step_orig(task, exc)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m curr_task \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     researcher \u001b[39m=\u001b[39m GPTResearcher(query\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery, report_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreport_type, source_urls\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, config_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     report \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m researcher\u001b[39m.\u001b[39mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# Write the report directly to report.txt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mreport.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m🔎 Running research for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Generate Agent\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrole \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m choose_agent(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLogs: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# If specified, the researcher will use the given urls as the context for the research.\u001b[39;00m\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mChooses the agent automatically\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    agent_role_prompt: Agent role prompt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m create_chat_completion(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         model\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39msmart_llm_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         messages\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mauto_agent_instructions()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtask: \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m}],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         llm_provider\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mllm_provider\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     agent_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m agent_dict[\u001b[39m\"\u001b[39m\u001b[39mserver\u001b[39m\u001b[39m\"\u001b[39m], agent_dict[\u001b[39m\"\u001b[39m\u001b[39magent_role_prompt\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# create response\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):  \u001b[39m# maximum of 10 attempts\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m send_chat_completion_request(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         messages, model, temperature, max_tokens, stream, llm_provider\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFailed to get response from OpenAI API\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb Cell 49\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39msend_chat_completion_request\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         messages, model, temperature, max_tokens, stream, llm_provider\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         result \u001b[39m=\u001b[39m lc_openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             model\u001b[39m=\u001b[39;49mmodel,  \u001b[39m# Change model here to use different models\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             provider\u001b[39m=\u001b[39;49mllm_provider,  \u001b[39m# Change provider here to use a different API\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search_and_retrieve_advanced_improved.ipynb#X66sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_community/adapters/openai.py:228\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(messages, provider, stream, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m converted_messages \u001b[39m=\u001b[39m convert_openai_messages(messages)\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 228\u001b[0m     result \u001b[39m=\u001b[39m model_config\u001b[39m.\u001b[39;49minvoke(converted_messages)\n\u001b[1;32m    229\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m: [{\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: convert_message_to_dict(result)}]}\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:164\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    160\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    161\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    163\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    165\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    166\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    167\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    168\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    169\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    170\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    171\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    172\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    173\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:495\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    489\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    493\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    494\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 495\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:382\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    381\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 382\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    383\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    384\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    385\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    386\u001b[0m ]\n\u001b[1;32m    387\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    370\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 372\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    373\u001b[0m                 m,\n\u001b[1;32m    374\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    375\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    376\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m         )\n\u001b[1;32m    379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    380\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:528\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    529\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:435\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    430\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    431\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    432\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m    433\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    434\u001b[0m }\n\u001b[0;32m--> 435\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    436\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    437\u001b[0m )\n\u001b[1;32m    438\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:352\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    354\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    356\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/openai/_utils/_utils.py:272\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/openai/resources/chat/completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    598\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    644\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 645\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    647\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    648\u001b[0m             {\n\u001b[1;32m    649\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    650\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    651\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    652\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    653\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    654\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    655\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    656\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    657\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    658\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    659\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    660\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    661\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    662\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    663\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    664\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    665\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    666\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    669\u001b[0m             },\n\u001b[1;32m    670\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    671\u001b[0m         ),\n\u001b[1;32m    672\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    673\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    674\u001b[0m         ),\n\u001b[1;32m    675\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    676\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    677\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    678\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    855\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    856\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    859\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    876\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    878\u001b[0m         request,\n\u001b[1;32m    879\u001b[0m         auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth,\n\u001b[1;32m    880\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    881\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    883\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    916\u001b[0m     request,\n\u001b[1;32m    917\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    918\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    919\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    944\u001b[0m         request,\n\u001b[1;32m    945\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    946\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1018\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Downloads/Agent_Agent/Search_Worflow_AgentAgent/search/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1170\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "user_input = \"Give me the database for everything to know about coffee. Make sure its extremely detailed\"\n",
    "message = get_completion(user_input, database_creation_agent, database_creation_tools, thread, client)\n",
    "wprint(f\"\\033[34m{database_creation_agent.name}: {message}\\033[0m\")\n",
    "\n",
    "# Load hierarchy from database.json\n",
    "with open('database.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize the processor with loaded data\n",
    "processor = HierarchicalSearchCreator(data)\n",
    "\n",
    "# Format and print the chunks\n",
    "formatted_chunks = processor.format_chunks()\n",
    "\n",
    "for chunk in formatted_chunks:\n",
    "    print(chunk)\n",
    "    write_to_file(chunk)\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    user_input = chunk\n",
    "    message = get_completion(user_input, report_generation_agent, report_generation_tools, thread, client)\n",
    "    wprint(f\"\\033[34m{report_generation_agent.name}: {message}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
